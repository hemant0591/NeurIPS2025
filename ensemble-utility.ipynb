{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":74608,"databundleVersionId":12609125,"isSourceIdPinned":false,"sourceType":"competition"}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%writefile req_kaggle.txt\n\nscikit-learn==1.7.0\nxgboost==3.0.2\nlightgbm==4.6.0\nnumpy==1.26.4\nscipy==1.14.1\npolars==1.31.0\nrdkit","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-27T08:00:11.482820Z","iopub.execute_input":"2025-06-27T08:00:11.483191Z","iopub.status.idle":"2025-06-27T08:00:11.496368Z","shell.execute_reply.started":"2025-06-27T08:00:11.483157Z","shell.execute_reply":"2025-06-27T08:00:11.495107Z"}},"outputs":[{"name":"stdout","text":"Writing req_kaggle.txt\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip download --q -r req_kaggle.txt -d /kaggle/working/packages","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-27T08:00:11.497628Z","iopub.execute_input":"2025-06-27T08:00:11.498020Z","iopub.status.idle":"2025-06-27T08:00:36.251562Z","shell.execute_reply.started":"2025-06-27T08:00:11.497987Z","shell.execute_reply":"2025-06-27T08:00:36.250290Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.9/12.9 MB\u001b[0m \u001b[31m60.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.9/253.9 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0mm\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m46.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m47.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.2/41.2 MB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.1/35.1 MB\u001b[0m \u001b[31m33.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.9/34.9 MB\u001b[0m \u001b[31m32.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.7/307.7 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.3/322.3 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0mm\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m28.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"%%writefile -a myimports.py\n\nprint(f\"\\n---> Commencing imports-part1\")\n\nfrom gc import collect\nfrom warnings import filterwarnings\nfilterwarnings('ignore')\nfrom IPython.display import display_html, clear_output\nclear_output()\nimport os, sys, logging, re, joblib, ctypes, shutil, random, torch\nfrom copy import deepcopy\n\nimport xgboost as xgb, lightgbm as lgb, catboost as cb, sklearn as sk, pandas as pd\nprint(f\"---> Sklearn = {sk.__version__}| Pandas = {pd.__version__}\")\ncollect()\n\nfrom os import path, walk, getpid\nfrom psutil import Process\nimport re\nfrom collections import Counter\nfrom itertools import product, combinations\n\nimport ctypes\nlibc = ctypes.CDLL(\"libc.so.6\")\n\nfrom functools import partial\nfrom copy import deepcopy\nimport numpy as np\n\nfrom scipy.spatial.transform import Rotation as R\nfrom scipy.stats import pearsonr\nimport polars as pl\nimport polars.selectors as cs\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom colorama import Fore, Style, init\nfrom typing import Tuple, List, Optional\n\nprint(\"Imports- part 1 done\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-27T08:00:36.254261Z","iopub.execute_input":"2025-06-27T08:00:36.254636Z","iopub.status.idle":"2025-06-27T08:00:36.263453Z","shell.execute_reply.started":"2025-06-27T08:00:36.254571Z","shell.execute_reply":"2025-06-27T08:00:36.262315Z"}},"outputs":[{"name":"stdout","text":"Writing myimports.py\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"%%writefile -a myimports.py\n\n# Pipeline specifics:-\nfrom sklearn.preprocessing import *\n\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import *\nfrom sklearn.inspection import permutation_importance\nfrom sklearn.feature_selection import VarianceThreshold as VT\nfrom sklearn.feature_extrection.text import TfidfVectorizer\nfrom sklearn.pipeline import Pipeline, make_pipeline\nfrom sklearn.base import BaseEstimator, TransformerMixin, clone\nfrom sklearn.compose import ColumnTransformer, make_column_selector\n\nfrom sklearn.metrics import *\n\nfrom xgboost import QuantileDMatrix, XGBClassifier as XGBC, XGBRegressor as XGBR\nfrom lightgbm import log_evaluation, early_stopping, LGBMClassifier as LGBMC, LGBMRegressor as LGBMR\nfrom catboost import CatBoostClassifier as CBC, Pool, CatBoostRegressor as CBR\nfrom sklearn.ensemble import HistGradientBoostingClassifier as HGBC, RandomForestClassifier as RFC\nfrom sklearn.ensemble import HistGradientBoostingRegressor as HGBR, RandomForestRegressor as RFR\nfrom sklearn.ensemble import VotingClassifier as VC, VotingRegressor as VR\nfrom sklearn.linear_model import LogisticRegression as LRC, Ridge, Lasso\nfrom sklearn.neighbors import KNeighborsClassifier as KNNC, KNeighborsRegressor as KNNR\n\n\nimport optuna\nfrom optuna import Trial, trial, create_study\nfrom optuna.pruners import HyperbandPruner\nfrom optuna.samplers import TPESampler, CmaEsSampler\n\nfrom rdkit import Chem\nfrom rdkit.Chem import AllChem\nfrom rdkit.Chem import Draw\nfrom rdkit.Chem import Descriptors\nfrom rdkit import DataStructs\nfrom rdkit import RDLogger  \nRDLogger.DisableLog('rdApp.*') \n\nsns.set({\"axes.facecolor\"       : \"white\",\n         \"figure.facecolor\"     : \"#ffffff\",\n         \"axes.edgecolor\"       : \"black\",\n         \"grid.color\"           : '#b0b0b0',\n         \"font.family\"          : ['Cambria'],\n         \"axes.labelcolor\"      : \"#000000\",\n         \"xtick.color\"          : \"#000000\",\n         \"ytick.color\"          : \"#000000\",\n         \"grid.linewidth\"       : 0.50,\n         \"grid.linestyle\"       : \"--\",\n         \"axes.titlecolor\"      : 'maroon',\n         'axes.titlesize'       : 9,\n         'axes.labelweight'     : \"bold\",\n         'legend.fontsize'      : 7.0,\n         'legend.title_fontsize': 7.0,\n         'font.size'            : 7.5,\n         'xtick.labelsize'      : 12.5,\n         'ytick.labelsize'      : 9.0,\n        }\n       )\n\ndef PrintColor(text: str, color = Fore.BLUE, style = Style.BRIGHT):\n    \"Prints color outputs using colorama using a text F-string\"\n    print(style + color + text + Style.RESET_ALL)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-27T08:00:36.264558Z","iopub.execute_input":"2025-06-27T08:00:36.265052Z","iopub.status.idle":"2025-06-27T08:00:36.291600Z","shell.execute_reply.started":"2025-06-27T08:00:36.265015Z","shell.execute_reply":"2025-06-27T08:00:36.290356Z"}},"outputs":[{"name":"stdout","text":"Appending to myimports.py\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"%%writefile -a myimports.py\n\nprint(\"Commencing imports-part2\")\n#optuna.logging.set_verbosity = optuna.logging.ERROR\noptuna.logging.set_verbosity(optuna.logging.ERROR)\noptuna.logging.disable_default_handler()\nprint(\"XGBoost = {xgb.__version__} | LightGBM = {lgb.__version__}\")\n\n\nclass MyLogger:\n    # to supress the logs in LGBM and Optuna (only logs when there's an error)\n    def __init__(self, logging_lbl):\n        self.logger = logging.getLogger(logging_lbl)\n        self.logger.setLevel(logging.ERROR)\n\n    def info(self, message): pass\n    def warning(self, message): pass\n    def error(self, message): self.logger.error(message)\n\nl = MyLogger(\"lightgbm_custom\")\nlgb.register_logger(l)\n\n# Customizing logging for XGBoost\nfor handler in logging.root.handlers[:]:\n    logging.root.removeHandler(handler)\n\nlogger = logging.getLogger(__name__)\nlogger.setLevel(logging.ERROR)\nformatter = logging.Formatter('%(asctime)s | %(levelname)s | %(message)s')\n\nstdout_handler = logging.StreamHandler(sys.stdout)\nstdout_handler.setLevel(logging.INFO)\nstdout_handler.setFormatter(formatter)\n\nfile_handler = logging.FileHandler(f'xgb_optimize.log')\nfile_handler.setLevel(logging.ERROR)\nfile_handler.setFormatter(formatter)\n\nlogger.addHandler(file_handler)\nlogger.addHandler(stdout_handler)\n\nclass XGBLogging(xgb.callback.TrainingCallback):\n    def __init__(self, epoch_log_interval=100):\n        self.epoch_log_interval = epoch_log_interval\n\n    def after_iteration(self, epoch, evals_log):\n        if self.epoch_log_interval <= 0:\n            pass\n\n        elif (epoch %  self.epoch_log_interval == 0):\n            for data, metric in evals_log.items():\n                for metric_name, log in metric.items():\n                    score = log[-1][0] if isinstance(log[-1], tuple) else log[-1]\n                    logger.info(f\"XGBLogging epoch {epoch} dataset {data} {metric_name} {score}\")\n\n        return False\n\nfrom sklearn import set_config\npd.set_option('display.max_columns', 1000)\npd.set_option('display.max_rows', 200)\nprint(\"Imports- part 2 done\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-27T08:00:36.292763Z","iopub.execute_input":"2025-06-27T08:00:36.293070Z","iopub.status.idle":"2025-06-27T08:00:36.315537Z","shell.execute_reply.started":"2025-06-27T08:00:36.293045Z","shell.execute_reply":"2025-06-27T08:00:36.314495Z"}},"outputs":[{"name":"stdout","text":"Appending to myimports.py\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"%%writefile -a myimports.py\n\nprint(\"Seeding everything\")\n\ndef seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n\nseed_everything(2024)\nprint(\"Imports done\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-27T08:00:36.316775Z","iopub.execute_input":"2025-06-27T08:00:36.317653Z","iopub.status.idle":"2025-06-27T08:00:36.345262Z","shell.execute_reply.started":"2025-06-27T08:00:36.317617Z","shell.execute_reply":"2025-06-27T08:00:36.343960Z"}},"outputs":[{"name":"stdout","text":"Appending to myimports.py\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"%%writefile -a myutils.py\n\nclass ParticipantVisibleError(Exception):\n    pass\n\nclass Utils:\n    def __init__(self):\n        self.minmax_tgt =  {\n            'Tg'      : [-148.0297376, 472.25],\n            'FFV'     : [0.2269924, 0.77709707],\n            'Tc'      : [0.0465, 0.524],\n            'Density' : [0.748691234, 1.840998909],\n            'Rg'      : [9.7283551, 34.672905605],\n        }\n        \n        self.NULL_FOR_SUBMISSION = -9999\n\n    def _scaling_error(self, labels, preds, prop):\n        \"\"\"\n        This function scales the errors based on the range of values of the columns\n        \"\"\"\n        error = np.abs(labels - preds)\n        label_range = self.minmax_tgt[prop][1] - self.minmax_tgt[prop][0]\n        return np.mean(error / label_range)\n\n    def _get_property_weights(self, labels):\n        \"\"\"\n        This function is computing relative weights for each target property, \n        based on how much valid data is available for that property.\n\n        This function gives higher weight to properties with fewer valid samples,\n        so that their contribution to the overall loss (or evaluation) \n        is not drowned out by properties with lots of data.\n        \"\"\"\n        weights = []\n        for prop in self.minmax_tgt.keys():\n            valid_rows = np.sum(labels[prop] != self.NULL_FOR_SUBMISSION)\n            weights.append(valid_rows)\n\n        weights = np.array(weights)\n        weights = np.sqrt(1 / weights)\n        return (weights / np.sum(weights)) * len(weights)\n\n    def ScoreMetric(self, df, preds_df, id_col=\"id\"):\n        props = self.minmax_tgt.keys()\n        props_maes = []\n        prop_weights = self._get_property_weights(df[props])\n\n        for prop in self.minmax_tgt.keys():\n            valid_rows = df[prop] != self.NULL_FOR_SUBMISSION\n            props_maes.append(self._scaling_error(df.loc[valid_rows, prop],\n                                                 preds_df.loc[valid_rows, prop], prop))\n        if len(props_maes) == 0: raise RuntimeError('No labels')\n        return np.float32(np.average(props_maes, weights=prop_weights))\n\n    def pp_preds(self, preds, target):\n        return np.clip(preds, \n                       a_min=self.minmax_tgt[target[0]],\n                       a_max=self.minmax_tgt[target[1]])\n\n    \n\n    def CleanMemory(self):\n        \"This method cleans the memory off unused objects and displays the cleaned state RAM usage\"\n\n        collect();\n        libc.malloc_trim(0)\n        pid = getpid()\n        py  = Process(pid)\n        memory_use = py.memory_info()[0] / 2. ** 30\n        return f\"\\nRAM usage = {memory_use :.4} GB\"\n\ncollect()\nprint()\n        ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-27T08:00:36.346744Z","iopub.execute_input":"2025-06-27T08:00:36.347081Z","iopub.status.idle":"2025-06-27T08:00:36.367917Z","shell.execute_reply.started":"2025-06-27T08:00:36.347045Z","shell.execute_reply":"2025-06-27T08:00:36.366668Z"}},"outputs":[{"name":"stdout","text":"Writing myutils.py\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"%%writefile -a mypp.py\n\nclass AdversarialCVMaker:\n    def __init__(self, n_splits=5):\n        model = LGBMC(n_estimators=200,\n                      learning_rate=0.02,\n                      max_depth=3,\n                      colsample_bytree=0.5,\n                      objective=\"binary\",\n                      metric=\"auc\",\n                      random_state=42,\n                      device=\"gpu\" if torch.cuda.is_available() else \"cpu\"\n                     )\n\n        self.n_splits = n_splits\n\n    @staticmethod\n    def scorer(ytrue, ypreds):\n        return roc_auc_score(ytrue, ypreds)\n\n    def make_cv(self, xtrain, xtest, **kwargs):\n        df = pd.concat([xtrain.assign(**(\"target\":0)), \n                        xtest.assign(**(\"target\":1))], axis=0,\n                        ignore_index=True)\n\n        cv = StratifiedKFold(n_splits=self.n_splits, random_state=42, shuffle=True)\n\n        scores = 0\n\n        for train_idx, val_idx in cv.split(df, df[\"target\"]):\n            X_train = df.loc[train_idx].drop(\"target\", axis=1)\n            X_val = df.loc[val_idx]\n            y_train = df.loc[train_idx, \"target\"]\n            y_val = df.loc[val_idx, \"target\"]\n\n            cat_cols = list(X_val.select_dtypes(include=[\"string\", \"category\", \"object\"]).colums)\n\n            if len(cat_cols) > 0:\n                X_train[cat_cols] = X_train[cat_cols].astype(\"category\")\n                X_val[cat_cols] = X_val[cat_cols].astype(\"category\")\n\n            else:\n                pass\n\n            model = clone(self.model) # fresh copy of the base model\n            model.fit(X_train, y_train)\n            preds = model.predict_proba(X_val)[:,1]\n            score = self.scorer(y_val, preds)\n            scores += score\n\n        score = scores / self.n_splits\n\n        PrintColor(\n            f\"\\n---> Overall adversarial CV score = {score :,.4f}\"\n        )\n\n        if score > 0.60 :\n            PrintColor(\n                f\"---> Check for test-train distribution shift\\n\", color = Fore.RED\n            )\n        else:\n            PrintColor(\n                f\"---> Train-test distributions are similar\\n\", color = Fore.GREEN\n            )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-27T08:00:36.369282Z","iopub.execute_input":"2025-06-27T08:00:36.369669Z","iopub.status.idle":"2025-06-27T08:00:36.392153Z","shell.execute_reply.started":"2025-06-27T08:00:36.369634Z","shell.execute_reply":"2025-06-27T08:00:36.390957Z"}},"outputs":[{"name":"stdout","text":"Writing mypp.py\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"%%writefile -a myapp.py\n\ndef reduce_mem_usage(df, df_name):\n    print(f\"reducing dataframe memory usage for {dataset}\")\n\n    initial_mem_usage = df.memory_usage().sum() / 1024**2\n\n    for col in tqdm(df.columns):\n        col_type = df[col].dtype\n\n        c_min = df[col].min()\n        c_max = df[col].max()\n\n        if str(col_type)[:3] == \"int\":\n            if c_min > np.iinfo(np.int8).min() and c_max < np.iinfo(np.int8).max():\n                df[col] = df[col].astype(np.int8)\n            elif c_min > np.iinfo(np.int16).min() and c_max < np.iinfo(np.int16).max():\n                df[col] = df[col].astype(np.int16)\n            elif c_min > np.iinfo(np.int32).min() and c_max < np.iinfo(np.int32).max():\n                df[col] = df[col].astype(np.int32)\n            elif c_min > np.iinfo(np.int64).min() and c_max < np.iinfo(np.int64).max():\n                df[col] = df[col].astype(np.int64)\n        else:\n            try:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n            except:\n                pass\n\n    final_mem_usage = df.memory_usage().sum() / 1024**2\n    print('---> Memory usage before: {:.2f} MB'.format(initial_mem_usage))\n    print('---> Memory usage after: {:.2f} MB'.format(final_mem_usage))\n\n    dec = np.float32(100 * (initial_mem_usage - final_mem_usage) / initial_mem_usage)\n    print(f'---> Decreased memory usage by {dec :.4f} percent \\n')\n    return df\n                    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-27T08:15:37.885735Z","iopub.execute_input":"2025-06-27T08:15:37.886193Z","iopub.status.idle":"2025-06-27T08:15:37.893960Z","shell.execute_reply.started":"2025-06-27T08:15:37.886167Z","shell.execute_reply":"2025-06-27T08:15:37.892835Z"}},"outputs":[{"name":"stdout","text":"Appending to myapp.py\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"%%writefile -a training.py\n\ndef permutation_importance_feats(model, X, y, split_grps, scorer, n_repeats=2, state=42, ntop=15, **kwargs):\n    cv = PredefinedSplit(split_grps)\n    n_splits = split_grps.nunique()\n    drop_cols = [\"Source\", \"id\", \"Id\", \"Label\", \"fold_nb\"]\n\n    for fold_nb, (train_idx, val_idx) in tqdm(enumerate(cv.split(X, y))):\n        X_train = X.loc[train_idx].drop(drop_cols, axis=1, errors=\"ignore\")\n        X_val = X.loc[val_idx].drop(drop_cols, axis=1, errors=\"ignore\")\n        y_train = y.loc[X_train.index]\n        y_val = y.loc[X_val.index]\n\n        model = clone(model)\n        selected_cols = list(X_val.columns)\n        model.fit(X_train, y_train)\n\n        imp_ = permutation_importance(model, X_val, y_val, scoring=scorer, n_repeats=n_repeats,\n                                      random_state=state)['importance_mean']\n\n    imp_ = pd.Series(index = sel_cols, data = imp_)\n\n    display(\n        imp_.\\\n        sort_values(ascending = False).\\\n        head(ntop).\\\n        to_frame().\\\n        transpose().\\\n        style.\\\n        format(formatter = '{:,.3f}').\\\n        background_gradient(\"icefire\", axis=1).\\\n        set_caption(f\"Top {ntop} features\")\n        )\n\n    return imp_","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class ModelTrainer:\n    def __init__(self,\n                 problem_type = 'regression',\n                 drop_cols =  [\"Source\", \"id\", \"Label\", \"fold_nb\"],\n                 len_train = 750_000,\n                 test_preds_req = True,\n                 target = \"\",\n                 pp_preds = True\n                )\n        self.problem_type = problem_type\n        self.drop_cols = drop_cols\n        self.len_train = len_train\n        self.test_preds_req = test_preds_req\n        self.target = target\n        self.pp_preds = pp_preds\n\n    def preprocess_data(self, df, method, cat_cols):\n        \"\"\"\n        preprocess data for categorical columns\n        \"\"\"\n        if cat_cols is not None:\n            if \"CB\" in method:\n                df[cat_cols] = df[cat_cols].astype(\"string\").fillna(\"missing\")\n            else:\n                df[cat_cols] = df[cat_cols].astype(\"string\").fillna(\"missing\").astype(\"category\")\n\n        return df.drop(self.drop_cols, axis=1, errors = \"ignore\")\n\n    def _postprocesspreds(self, ypreds):\n        \"Optional post-processing method for predictions\"\n\n        if self.pp_preds :\n            minmax_tgt =  {\n                'Tg'      : [-148.0297376, 472.25],\n                'FFV'     : [0.2269924, 0.77709707],\n                'Tc'      : [0.0465, 0.524],\n                'Density' : [0.748691234, 1.840998909],\n                'Rg'      : [9.7283551, 34.672905605],\n            }\n\n            return np.clip( \n                ypreds , \n                a_min = minmax_tgt[self.target][0], \n                a_max = minmax_tgt[self.target][1]\n            )\n        else:\n            return ypreds\n\n    def predict(self, X, model):\n        if self.problem_type == 'regression':\n            return pd.DataFrame(self._postprocesspreds(model.predict(X)), index = X.index)\n\n        elif self.problem_type == 'binary':\n            return pd.DataFrame(self._postprocesspreds(model.predict_proba(X)[:,1]), index = X.index)\n\n        elif self.problem_type == 'multiclass':\n            return pd.DataFrame(self._postprocesspreds(model.predict_proba(X)), index = X.index)\n\n\n    def _collate_preds(self, preds):\n        return pd.concat(preds, axis=0, ignore_index=False).groupby(level=0).mean().sort_index(ascending=True)\n\n    def score(self, ytrue, ypreds):\n        \"Calculates the relevant OOF score for the given problem type\"\n        \n        score = mean_absolute_error(ytrue, ypreds)\n        return score \n\n    def _get_best_iter(self, model, method):\n        best_iter = 0\n        try:\n            if \"XGB\" in method: best_iter = model.best_iteration\n            if \"LGB\" in method: best_iter = model.best_iteration_\n            if \"CB\" in method: best_iter = model.get_best_iteration()\n        except:\n            pass\n\n        return best_iter\n\n    def fit_predict(self,\n                    Xtrain,\n                    ytrain,\n                    Xtest,\n                    split_grps,\n                    extra,\n                    method,\n                    model,\n                    cat_cols,\n                    **kwargs\n                   ):\n        oof_preds, model_preds, fitted_models, best_iter = [], [], [], []\n        cv = PredefinedSplit(split_grps)\n        PrintColor(f\"\\n {method} offline model training\" )\n\n        if extra is not None :\n            print(f\"---> Added extra data in full across each fold\")\n\n        for fold_nb, (train_idx, val_idx) in tqdm(enumerate(cv.split(Xtrain, ytrain), start=1), method):\n            X_train = Xtrain.loc[train_idx]\n            X_val = Xtrain.loc[val_idx]\n            y_train = ytrain[train_idx]\n            y_val = ytrain[val_idx]\n\n            if extra is not None:\n                X_train = pd.concat([X_train, extra[0][X_train.columns]], axis=0, ignore_index=True)\n                y_train = pd.concat([y_train, extra[1]], axis=0, ignore_index=True)\n\n            X_train = self.preprocess_data(X_train, method, cat_cols)\n            X_val = self.preprocess_data(X_val, method, cat_cols)\n\n            if self.test_preds_req:\n                Xtest = self.preprocess_data(Xtest, method, cat_cols)\n\n            model = clone(model)\n\n            try:\n                model.fit(X_train, y_train, eval_set=[(X_val,yval)], **kwargs)\n            except:\n                mode.fit(X_train, y_train, **kwargs)\n\n            fitted_models.append(model)\n\n            try: best_iter.append(self._get_best_iter(model,method))\n            except: pass\n\n            val_preds = model.predict(X_val)\n            oof_preds.append(val_preds)\n\n            if self.problem_type == \"multiclass\":\n                score, bscore, mscore = self.score(y_val.values, np.argmax(val_preds.values, axis=1) )\n                print(\n                    f\"---> Score = {score:,.8f} | Binary = {bscore:,.8f} | Multiclass = {mscore:,.8f} -- Fold {fold_nb}\"\n                )\n            else:\n                score = self.score(y_val.values, val_preds.values)\n                print(f\"---> Score = {score:,.8f} -- Fold {fold_nb}\")\n\n            if self.test_preds_req :\n                test_preds = self.predict(Xtest, model)\n                model_preds.append(test_preds)\n\n        oof_preds = self._collate_preds(oof_preds)\n\n        if self.test_preds_req:\n            model_preds = self._collate_preds(model_preds)\n        else:\n            print(\"Model predictions not required\")\n\n        try:\n            if np.amax(best_iter) > 0 :\n                print(f\"\\n---> Maximum early stopping iteration = {np.amax(best_iter):.0f}\")\n            else:\n                pass\n        except:\n            pass\n\n        if self.problem_type == \"multiclass\":\n            score, bscore, mscore = self.score(\n                ytrain.values.flatten(), \n                oof_preds.idxmax(axis=1).values.flatten() \n            )\n            print(\n                f\"\\n---> Overall Score = {score:,.8f} | Binary = {bscore:,.8f} | Multiclass = {mscore:,.8f}\\n\"\n            )\n        else:\n            score = self.score(y_train.values, oof_preds.values.flatten())\n            print(f\"\\n---> Overall Score = {score:,.8f}\\n\")\n        \n        return (fitted_models, oof_preds, model_preds)\n\n    \n\n    \n    ","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}