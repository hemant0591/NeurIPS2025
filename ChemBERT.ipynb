{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "12bfad66-0349-4aab-8f5e-53f3b6ddf7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import gc\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "\n",
    "from transformers import RobertaModel, RobertaTokenizer, get_linear_schedule_with_warmup\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "039a320c-7542-4007-bad8-c5bb015a1915",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cdf2b604-80ad-45f7-aa4b-ba6de8069e99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7973, 7)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7c7c697e-221d-48f6-9020-79f3bbe229e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>SMILES</th>\n",
       "      <th>Tg</th>\n",
       "      <th>FFV</th>\n",
       "      <th>Tc</th>\n",
       "      <th>Density</th>\n",
       "      <th>Rg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>87817</td>\n",
       "      <td>*CC(*)c1ccccc1C(=O)OCCCCCC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.374645</td>\n",
       "      <td>0.205667</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>106919</td>\n",
       "      <td>*Nc1ccc([C@H](CCC)c2ccc(C3(c4ccc([C@@H](CCC)c5...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.370410</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>388772</td>\n",
       "      <td>*Oc1ccc(S(=O)(=O)c2ccc(Oc3ccc(C4(c5ccc(Oc6ccc(...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.378860</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>519416</td>\n",
       "      <td>*Nc1ccc(-c2c(-c3ccc(C)cc3)c(-c3ccc(C)cc3)c(N*)...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.387324</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>539187</td>\n",
       "      <td>*Oc1ccc(OC(=O)c2cc(OCCCCCCCCCOCC3CCCN3c3ccc([N...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.355470</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id                                             SMILES  Tg       FFV  \\\n",
       "0   87817                         *CC(*)c1ccccc1C(=O)OCCCCCC NaN  0.374645   \n",
       "1  106919  *Nc1ccc([C@H](CCC)c2ccc(C3(c4ccc([C@@H](CCC)c5... NaN  0.370410   \n",
       "2  388772  *Oc1ccc(S(=O)(=O)c2ccc(Oc3ccc(C4(c5ccc(Oc6ccc(... NaN  0.378860   \n",
       "3  519416  *Nc1ccc(-c2c(-c3ccc(C)cc3)c(-c3ccc(C)cc3)c(N*)... NaN  0.387324   \n",
       "4  539187  *Oc1ccc(OC(=O)c2cc(OCCCCCCCCCOCC3CCCN3c3ccc([N... NaN  0.355470   \n",
       "\n",
       "         Tc  Density  Rg  \n",
       "0  0.205667      NaN NaN  \n",
       "1       NaN      NaN NaN  \n",
       "2       NaN      NaN NaN  \n",
       "3       NaN      NaN NaN  \n",
       "4       NaN      NaN NaN  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "38a34c64-1cc0-418e-85ef-46efe75e494c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id            0\n",
       "SMILES        0\n",
       "Tg         7462\n",
       "FFV         943\n",
       "Tc         7236\n",
       "Density    7360\n",
       "Rg         7359\n",
       "dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e7698d04-79ca-4bae-958f-af726b1769c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>SMILES</th>\n",
       "      <th>Tg</th>\n",
       "      <th>FFV</th>\n",
       "      <th>Tc</th>\n",
       "      <th>Density</th>\n",
       "      <th>Rg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>7.973000e+03</td>\n",
       "      <td>7973</td>\n",
       "      <td>511.000000</td>\n",
       "      <td>7030.000000</td>\n",
       "      <td>737.000000</td>\n",
       "      <td>613.000000</td>\n",
       "      <td>614.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>NaN</td>\n",
       "      <td>7973</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>NaN</td>\n",
       "      <td>*CC(*)c1ccccc1C(=O)OCCCCCC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.080050e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>96.452314</td>\n",
       "      <td>0.367212</td>\n",
       "      <td>0.256334</td>\n",
       "      <td>0.985484</td>\n",
       "      <td>16.419787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>6.218241e+08</td>\n",
       "      <td>NaN</td>\n",
       "      <td>111.228279</td>\n",
       "      <td>0.029609</td>\n",
       "      <td>0.089538</td>\n",
       "      <td>0.146189</td>\n",
       "      <td>4.608640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>8.781700e+04</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-148.029738</td>\n",
       "      <td>0.226992</td>\n",
       "      <td>0.046500</td>\n",
       "      <td>0.748691</td>\n",
       "      <td>9.728355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>5.376641e+08</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13.674509</td>\n",
       "      <td>0.349549</td>\n",
       "      <td>0.186000</td>\n",
       "      <td>0.890243</td>\n",
       "      <td>12.540328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.079079e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>74.040183</td>\n",
       "      <td>0.364264</td>\n",
       "      <td>0.236000</td>\n",
       "      <td>0.948193</td>\n",
       "      <td>15.052194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.621708e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>161.147595</td>\n",
       "      <td>0.380790</td>\n",
       "      <td>0.330500</td>\n",
       "      <td>1.062096</td>\n",
       "      <td>20.411067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.147438e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>472.250000</td>\n",
       "      <td>0.777097</td>\n",
       "      <td>0.524000</td>\n",
       "      <td>1.840999</td>\n",
       "      <td>34.672906</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  id                      SMILES          Tg          FFV  \\\n",
       "count   7.973000e+03                        7973  511.000000  7030.000000   \n",
       "unique           NaN                        7973         NaN          NaN   \n",
       "top              NaN  *CC(*)c1ccccc1C(=O)OCCCCCC         NaN          NaN   \n",
       "freq             NaN                           1         NaN          NaN   \n",
       "mean    1.080050e+09                         NaN   96.452314     0.367212   \n",
       "std     6.218241e+08                         NaN  111.228279     0.029609   \n",
       "min     8.781700e+04                         NaN -148.029738     0.226992   \n",
       "25%     5.376641e+08                         NaN   13.674509     0.349549   \n",
       "50%     1.079079e+09                         NaN   74.040183     0.364264   \n",
       "75%     1.621708e+09                         NaN  161.147595     0.380790   \n",
       "max     2.147438e+09                         NaN  472.250000     0.777097   \n",
       "\n",
       "                Tc     Density          Rg  \n",
       "count   737.000000  613.000000  614.000000  \n",
       "unique         NaN         NaN         NaN  \n",
       "top            NaN         NaN         NaN  \n",
       "freq           NaN         NaN         NaN  \n",
       "mean      0.256334    0.985484   16.419787  \n",
       "std       0.089538    0.146189    4.608640  \n",
       "min       0.046500    0.748691    9.728355  \n",
       "25%       0.186000    0.890243   12.540328  \n",
       "50%       0.236000    0.948193   15.052194  \n",
       "75%       0.330500    1.062096   20.411067  \n",
       "max       0.524000    1.840999   34.672906  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4cba4416-4b32-412e-a500-79d858349c27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tg         7462\n",
       "FFV         943\n",
       "Tc         7236\n",
       "Density    7360\n",
       "Rg         7359\n",
       "dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing = train_df.isnull().sum()[['Tg', 'FFV', 'Tc', 'Density', 'Rg']]\n",
    "missing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2107c8-0431-4534-bddc-263153c148a5",
   "metadata": {},
   "source": [
    "### Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "401d148a-b028-40bc-beb0-557913c5f52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SMILESDataset(Dataset):\n",
    "    def __init__(self, df, target_col=None):\n",
    "        self.smiles = df[\"SMILES\"].tolist()\n",
    "        self.has_target = target_col is not None\n",
    "        if self.has_target:\n",
    "            self.targets = df[target_col].values.astype(\"float32\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.smiles)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {\"smiles\": self.smiles[idx]}\n",
    "        if self.has_target:\n",
    "            item[\"target\"] = self.targets[idx]\n",
    "        return item\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2e0456d7-373c-41f9-b1d9-8564a3050587",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chemberta_collate_fn(batch, tokenizer):\n",
    "    smiles = [item[\"smiles\"] for item in batch]\n",
    "    encoding = tokenizer(smiles, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "    result = {\n",
    "        \"input_ids\": encoding[\"input_ids\"],\n",
    "        \"attention_mask\": encoding[\"attention_mask\"]\n",
    "    }\n",
    "    if \"target\" in batch[0]:\n",
    "        targets = torch.tensor([item[\"target\"] for item in batch], dtype=torch.float32)\n",
    "        result[\"targets\"] = targets\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16408336-baf1-4941-a6d9-b3d8db7a898a",
   "metadata": {},
   "source": [
    "### Writing model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e36e5278-2ac1-42c5-9108-696ffa356b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "class chemBERTaModel(nn.Module):\n",
    "    def __init__(self, base_model, out_dim=1):\n",
    "        super().__init__()\n",
    "        self.base_model = base_model\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.regressor = nn.Linear(base_model.config.hidden_size, out_dim)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.base_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls_tokens = outputs.last_hidden_state[:,0]\n",
    "        return self.regressor(self.dropout(cls_tokens)).squeeze(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb807b33-a77a-4159-8395-d3938689d087",
   "metadata": {},
   "source": [
    "### Training and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "34984de0-4174-4218-ae62-184878ffac2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dl, loss_fn, opt, sched):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    n = 0\n",
    "    for batch in dl:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        targets = batch['targets'].to(device)\n",
    "        preds = model(input_ids, attention_mask)\n",
    "        loss = loss_fn(preds, targets)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        sched.step()\n",
    "        opt.zero_grad()\n",
    "        total_loss += loss.item() * len(targets)\n",
    "        n += len(targets)\n",
    "    return total_loss / n\n",
    "\n",
    "def eval(model, dl, loss_fn):\n",
    "    model.eval()\n",
    "    total_loss, n = 0, 0\n",
    "    all_preds, all_targs = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in dl:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            targets = batch['targets'].to(device)\n",
    "            preds = model(input_ids, attention_mask)\n",
    "            loss = loss_fn(preds, targets)\n",
    "            total_loss += loss.item() * len(targets)\n",
    "            n += len(targets)\n",
    "            all_preds.extend(preds.detach().cpu().numpy())\n",
    "            all_targs.extend(targets.detach().cpu().numpy())\n",
    "    return total_loss / n, all_preds, all_targs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afffa0ec-1c81-4c3a-ab66-7998b44f27a6",
   "metadata": {},
   "source": [
    "### Training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7463c4ba-1980-4584-858f-8ef0bc811f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_chemberta(df, target, base_model, tokenizer, n_epochs=30, save_dir = 'saved_models_chemberta', patience=5):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    df_clean = df[['SMILES', target]].dropna()\n",
    "        \n",
    "    # scale target\n",
    "    y_scaler = StandardScaler()\n",
    "    df_clean[target] = y_scaler.fit_transform(df_clean[[target]])\n",
    "    \n",
    "    # save scaler\n",
    "    with open(os.path.join(save_dir, f\"{target}_scaler.pkl\"), 'wb') as f:\n",
    "        pickle.dump(y_scaler, f)\n",
    "        \n",
    "    train_data, val_data = train_test_split(df_clean, test_size=0.2, random_state=42)\n",
    "    train_ds = SMILESDataset(train_data, target)\n",
    "    val_ds = SMILESDataset(val_data, target)\n",
    "    train_dl = DataLoader(train_ds, batch_size=16, shuffle=True, collate_fn=lambda x: chemberta_collate_fn(x, tokenizer))\n",
    "    val_dl = DataLoader(val_ds, batch_size=16, shuffle=False, collate_fn=lambda x: chemberta_collate_fn(x, tokenizer))\n",
    "    \n",
    "    model = chemBERTaModel(base_model).to(device)\n",
    "    opt = AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        opt,\n",
    "        num_warmup_steps=int(0.1 * len(train_dl) * n_epochs),\n",
    "        num_training_steps=len(train_dl) * n_epochs\n",
    "    )\n",
    "    loss_fn = nn.MSELoss()\n",
    "    \n",
    "    best_val_loss = float(\"inf\")\n",
    "    epoch_no_improve = 0\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        train_loss = train(model, train_dl, loss_fn, opt, scheduler)\n",
    "        val_loss, preds, targs = eval(model, val_dl, loss_fn)\n",
    "        print(f\"Epoch {epoch+1}/{n_epochs} | Train MAE: {train_loss:.4f}, Val MAE: {val_loss:.4f}\")\n",
    "        \n",
    "        if val_loss <= best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), os.path.join(save_dir, f\"{target}_model.pt\"))\n",
    "            epoch_no_improve = 0\n",
    "        else:\n",
    "            epoch_no_improve += 1\n",
    "            if epoch_no_improve >= patience:\n",
    "                print(\"Early stopping triggered!\")\n",
    "                break\n",
    "        \n",
    "    model.load_state_dict(torch.load(os.path.join(save_dir, f\"{target}_model.pt\")))\n",
    "    \n",
    "    # Free up memory\n",
    "    # del train_dl, val_dl, opt, scheduler\n",
    "    # torch.cuda.empty_cache()\n",
    "    # gc.collect()\n",
    "    \n",
    "    return model, tokenizer, y_scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "537df94b-d1ce-4210-aaa6-1b7aa2eeb7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_chemberta(df, target, model, tokenizer, scaler):\n",
    "    test_ds = SMILESDataset(df)\n",
    "    test_loader = DataLoader(test_ds, batch_size=16, shuffle=False, collate_fn=lambda x: chemberta_collate_fn(x, tokenizer))\n",
    "    \n",
    "    model.eval()\n",
    "    preds = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            preds.extend(outputs.detach().cpu().numpy())\n",
    "    preds = scaler.inverse_transform(np.array(preds).reshape(-1, 1)).flatten()\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "545233f9-a64b-4330-a7a2-fccf5fac24a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on predicting: Tg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at seyonec/ChemBERTa-zinc-base-v1 were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.decoder.bias', 'lm_head.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30 | Train MAE: 1.1250, Val MAE: 0.6074\n",
      "Epoch 2/30 | Train MAE: 0.7552, Val MAE: 0.5187\n",
      "Epoch 3/30 | Train MAE: 0.5611, Val MAE: 0.4262\n",
      "Epoch 4/30 | Train MAE: 0.5084, Val MAE: 0.7141\n",
      "Epoch 5/30 | Train MAE: 0.3966, Val MAE: 0.4734\n",
      "Epoch 6/30 | Train MAE: 0.3594, Val MAE: 0.5363\n",
      "Epoch 7/30 | Train MAE: 0.2882, Val MAE: 0.4919\n",
      "Epoch 8/30 | Train MAE: 0.2733, Val MAE: 0.6605\n",
      "Early stopping triggered!\n",
      "Working on predicting: FFV\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at seyonec/ChemBERTa-zinc-base-v1 were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.decoder.bias', 'lm_head.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30 | Train MAE: 0.8305, Val MAE: 0.6322\n",
      "Epoch 2/30 | Train MAE: 0.4987, Val MAE: 0.5057\n",
      "Epoch 3/30 | Train MAE: 0.3944, Val MAE: 0.4109\n",
      "Epoch 4/30 | Train MAE: 0.2914, Val MAE: 0.3281\n",
      "Epoch 5/30 | Train MAE: 0.2156, Val MAE: 0.3172\n",
      "Epoch 6/30 | Train MAE: 0.1728, Val MAE: 0.3212\n",
      "Epoch 7/30 | Train MAE: 0.1390, Val MAE: 0.3040\n",
      "Epoch 8/30 | Train MAE: 0.1160, Val MAE: 0.2771\n",
      "Epoch 9/30 | Train MAE: 0.0988, Val MAE: 0.2770\n",
      "Epoch 10/30 | Train MAE: 0.0859, Val MAE: 0.2625\n",
      "Epoch 11/30 | Train MAE: 0.0766, Val MAE: 0.2976\n",
      "Epoch 12/30 | Train MAE: 0.0678, Val MAE: 0.2749\n",
      "Epoch 13/30 | Train MAE: 0.0614, Val MAE: 0.2734\n",
      "Epoch 14/30 | Train MAE: 0.0570, Val MAE: 0.2667\n",
      "Epoch 15/30 | Train MAE: 0.0529, Val MAE: 0.2430\n",
      "Epoch 16/30 | Train MAE: 0.0509, Val MAE: 0.2588\n",
      "Epoch 17/30 | Train MAE: 0.0462, Val MAE: 0.2455\n",
      "Epoch 18/30 | Train MAE: 0.0433, Val MAE: 0.2627\n",
      "Epoch 19/30 | Train MAE: 0.0421, Val MAE: 0.2423\n",
      "Epoch 20/30 | Train MAE: 0.0387, Val MAE: 0.2434\n",
      "Epoch 21/30 | Train MAE: 0.0368, Val MAE: 0.2403\n",
      "Epoch 22/30 | Train MAE: 0.0359, Val MAE: 0.2433\n",
      "Epoch 23/30 | Train MAE: 0.0323, Val MAE: 0.2450\n",
      "Epoch 24/30 | Train MAE: 0.0317, Val MAE: 0.2435\n",
      "Epoch 25/30 | Train MAE: 0.0294, Val MAE: 0.2576\n",
      "Epoch 26/30 | Train MAE: 0.0286, Val MAE: 0.2432\n",
      "Early stopping triggered!\n",
      "Working on predicting: Tc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at seyonec/ChemBERTa-zinc-base-v1 were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.decoder.bias', 'lm_head.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30 | Train MAE: 1.4107, Val MAE: 0.5250\n",
      "Epoch 2/30 | Train MAE: 0.5542, Val MAE: 0.3744\n",
      "Epoch 3/30 | Train MAE: 0.4199, Val MAE: 0.3258\n",
      "Epoch 4/30 | Train MAE: 0.3220, Val MAE: 0.2806\n",
      "Epoch 5/30 | Train MAE: 0.3231, Val MAE: 0.3090\n",
      "Epoch 6/30 | Train MAE: 0.3032, Val MAE: 0.3074\n",
      "Epoch 7/30 | Train MAE: 0.2595, Val MAE: 0.3032\n",
      "Epoch 8/30 | Train MAE: 0.2469, Val MAE: 0.2873\n",
      "Epoch 9/30 | Train MAE: 0.2256, Val MAE: 0.2844\n",
      "Early stopping triggered!\n",
      "Working on predicting: Density\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at seyonec/ChemBERTa-zinc-base-v1 were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.decoder.bias', 'lm_head.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30 | Train MAE: 1.0536, Val MAE: 0.6110\n",
      "Epoch 2/30 | Train MAE: 0.7440, Val MAE: 0.5615\n",
      "Epoch 3/30 | Train MAE: 0.4873, Val MAE: 0.5061\n",
      "Epoch 4/30 | Train MAE: 0.4030, Val MAE: 0.4311\n",
      "Epoch 5/30 | Train MAE: 0.2941, Val MAE: 0.4391\n",
      "Epoch 6/30 | Train MAE: 0.2740, Val MAE: 0.4278\n",
      "Epoch 7/30 | Train MAE: 0.2082, Val MAE: 0.4592\n",
      "Epoch 8/30 | Train MAE: 0.2388, Val MAE: 0.4236\n",
      "Epoch 9/30 | Train MAE: 0.2068, Val MAE: 0.3969\n",
      "Epoch 10/30 | Train MAE: 0.2280, Val MAE: 0.4500\n",
      "Epoch 11/30 | Train MAE: 0.1920, Val MAE: 0.4656\n",
      "Epoch 12/30 | Train MAE: 0.1607, Val MAE: 0.4359\n",
      "Epoch 13/30 | Train MAE: 0.1849, Val MAE: 0.4206\n",
      "Epoch 14/30 | Train MAE: 0.1526, Val MAE: 0.4202\n",
      "Early stopping triggered!\n",
      "Working on predicting: Rg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at seyonec/ChemBERTa-zinc-base-v1 were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.decoder.bias', 'lm_head.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30 | Train MAE: 0.9601, Val MAE: 0.4952\n",
      "Epoch 2/30 | Train MAE: 0.5452, Val MAE: 0.3819\n",
      "Epoch 3/30 | Train MAE: 0.3823, Val MAE: 0.3634\n",
      "Epoch 4/30 | Train MAE: 0.3216, Val MAE: 0.4213\n",
      "Epoch 5/30 | Train MAE: 0.3087, Val MAE: 0.3456\n",
      "Epoch 6/30 | Train MAE: 0.2633, Val MAE: 0.3221\n",
      "Epoch 7/30 | Train MAE: 0.2258, Val MAE: 0.3475\n",
      "Epoch 8/30 | Train MAE: 0.2016, Val MAE: 0.3525\n",
      "Epoch 9/30 | Train MAE: 0.2218, Val MAE: 0.3237\n",
      "Epoch 10/30 | Train MAE: 0.2101, Val MAE: 0.3274\n",
      "Epoch 11/30 | Train MAE: 0.1937, Val MAE: 0.3188\n",
      "Epoch 12/30 | Train MAE: 0.1742, Val MAE: 0.3265\n",
      "Epoch 13/30 | Train MAE: 0.1771, Val MAE: 0.3133\n",
      "Epoch 14/30 | Train MAE: 0.1695, Val MAE: 0.3458\n",
      "Epoch 15/30 | Train MAE: 0.1683, Val MAE: 0.3472\n",
      "Epoch 16/30 | Train MAE: 0.1575, Val MAE: 0.3346\n",
      "Epoch 17/30 | Train MAE: 0.1528, Val MAE: 0.3632\n",
      "Epoch 18/30 | Train MAE: 0.1328, Val MAE: 0.3224\n",
      "Early stopping triggered!\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"seyonec/ChemBERTa-zinc-base-v1\"\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "targets = ['Tg', 'FFV', 'Tc', 'Density', 'Rg']\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "for target in targets:\n",
    "    print(f'Working on predicting: {target}')\n",
    "    \n",
    "    base_model = RobertaModel.from_pretrained(MODEL_NAME)\n",
    "    \n",
    "    model, tokenizer, y_scaler = train_chemberta(\n",
    "        df=train_df, \n",
    "        target=target, \n",
    "        base_model=base_model, \n",
    "        tokenizer=tokenizer)\n",
    "    \n",
    "    test_df[target] = predict_chemberta(\n",
    "        df=test_df, \n",
    "        target=target, \n",
    "        model=model, \n",
    "        tokenizer=tokenizer, \n",
    "        scaler=y_scaler\n",
    "    )\n",
    "    \n",
    "    # Clean up to be safe\n",
    "    del model, base_model, y_scaler\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "62858d6b-73d7-4df1-87e4-be05cf67d96c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>SMILES</th>\n",
       "      <th>Tg</th>\n",
       "      <th>FFV</th>\n",
       "      <th>Tc</th>\n",
       "      <th>Density</th>\n",
       "      <th>Rg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1109053969</td>\n",
       "      <td>*Oc1ccc(C=NN=Cc2ccc(Oc3ccc(C(c4ccc(*)cc4)(C(F)...</td>\n",
       "      <td>117.830582</td>\n",
       "      <td>0.369690</td>\n",
       "      <td>0.244372</td>\n",
       "      <td>1.186184</td>\n",
       "      <td>21.222961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1422188626</td>\n",
       "      <td>*Oc1ccc(C(C)(C)c2ccc(Oc3ccc(C(=O)c4cccc(C(=O)c...</td>\n",
       "      <td>155.775955</td>\n",
       "      <td>0.376675</td>\n",
       "      <td>0.297824</td>\n",
       "      <td>1.141359</td>\n",
       "      <td>21.309484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2032016830</td>\n",
       "      <td>*c1cccc(OCCCCCCCCOc2cccc(N3C(=O)c4ccc(-c5cccc6...</td>\n",
       "      <td>159.235245</td>\n",
       "      <td>0.343452</td>\n",
       "      <td>0.280604</td>\n",
       "      <td>1.128601</td>\n",
       "      <td>25.005985</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id                                             SMILES          Tg  \\\n",
       "0  1109053969  *Oc1ccc(C=NN=Cc2ccc(Oc3ccc(C(c4ccc(*)cc4)(C(F)...  117.830582   \n",
       "1  1422188626  *Oc1ccc(C(C)(C)c2ccc(Oc3ccc(C(=O)c4cccc(C(=O)c...  155.775955   \n",
       "2  2032016830  *c1cccc(OCCCCCCCCOc2cccc(N3C(=O)c4ccc(-c5cccc6...  159.235245   \n",
       "\n",
       "        FFV        Tc   Density         Rg  \n",
       "0  0.369690  0.244372  1.186184  21.222961  \n",
       "1  0.376675  0.297824  1.141359  21.309484  \n",
       "2  0.343452  0.280604  1.128601  25.005985  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b49d8885-0d63-46fb-bbb5-6238717390de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10    28.682441\n",
       "11    13.534248\n",
       "37    13.872913\n",
       "46    12.737463\n",
       "64    13.435339\n",
       "Name: Rg, dtype: float64"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['Rg'].dropna().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d55ac4-a4b0-488d-b87d-940a719aa77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lengths = train_df['SMILES'].apply(lambda x: len(tokenizer.tokenize(x)))\n",
    "# print(lengths.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb6aec4-2fbc-4200-b62a-2263fd64cd67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
